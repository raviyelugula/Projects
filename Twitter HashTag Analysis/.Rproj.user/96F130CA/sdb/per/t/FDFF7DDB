{
    "collab_server" : "",
    "contents" : "---\ntitle: \"CokeVsPepsi\"\nauthor: \"Ravindranadh\"\ndate: \"16 September 2017\"\noutput: html_document\n---\n\n###1.Introduction\nWe focused on fetching the tweets with `CocaCola` and `Pepsi` from twitter and analyse their sentiments.\n\nFollowing are the necesssary libraries that are needed.\n```{r message=FALSE,warning=FALSE}\nif (Sys.getenv(\"JAVA_HOME\")!=\"\") Sys.setenv(JAVA_HOME=\"\")\nrequire(twitteR)\nrequire(tm)\nrequire(stringr)\nrequire(qdap)\nrequire(ggplot2)\nrequire(wordcloud)\nrequire(topicmodels)\nrequire(tidytext)\nrequire(dplyr)\nrequire(plotrix)\nrequire(ggthemes)\n```\n\n###2.Data Fetching\n* Define your twitter authentication details (this part of code is hidden)\n```{r echo=FALSE}\n  consumerkey = \"MTfL0uv6oBbtVBa3dPuR4PS51\"\n  consumersecret = \"T480vrOtvSK1XxGcPp20SdRoW99by3EGVvAapzyq8io0uH6lUh\"\n  accesstoken = \"191958252-uOisKMkvWHWmiTRQnf9d6XMww5wtZVtox1ShcjMm\"\n  accesssecret = \"9kTTYtORX4KnXk9SG4R9bQ1PpmQyBYEwmfvDksTh6Wht0\"\n```\n\n* Establish a connect with twitter developer API by passing the authentication details to `setup_twitter_oauth` function.\n* Pull recent 2,500 tweets of each search\n* Store the tweets in a dataframe and convert it to corpus.\n```{r message=FALSE, results='hide'}\nsetup_twitter_oauth(consumerkey,consumersecret,accesstoken,accesssecret)\nCocaCola_TweetsList= searchTwitter(searchString ='Coke',n=10000 , lang = 'en') \nPepsi_TweetsList= searchTwitter(searchString ='Pepsi',n=10000 , lang = 'en') \n\nCocaCola_DF = twListToDF(CocaCola_TweetsList)\nPepsi_DF = twListToDF(Pepsi_TweetsList)\n\nCocaCola_DF$createdDate = as.Date(CocaCola_DF$created,format = \"%d-%m-%Y\")\nPepsi_DF$createdDate = as.Date(Pepsi_DF$created,format = \"%d-%m-%Y\")\n\nCocaCola_Tweets = as.character(CocaCola_DF$text)\nCocaCola_Corpus = Corpus(VectorSource(CocaCola_Tweets))\n\nPepsi_Tweets = as.character(Pepsi_DF$text)\nPepsi_Corpus = Corpus(VectorSource(Pepsi_Tweets))\n```\n\n###3.Data Cleaning\n* The corpus is subjected to cleaning by removing the `usernames` that are followed by **@**,\n* `URLs` are removed.\n* `Pure Numbers` i.e like 54 but not iphone8 (aplha-numeric), are removed.\n* `Puctuations & whitespaces` are removed.\n* Entire text is converted into `lower case`.\n* `Abbrevations` are replaced.\n* All the words are subjected to `stemming`.\n* `Stopwords` are removed.\n```{r results='hide',message=FALSE}\nremoveUsernameInRT <- function(x) str_replace_all(x,\"(?<=@)[^\\\\s:]+\",'')\nTweets_Corpus = tm_map(Tweets_Corpus, content_transformer(removeUsernameInRT))\n\nremoveURL <- function(x) gsub(\"http[^[:space:]]*\", \"\", x)\nTweets_Corpus = tm_map(Tweets_Corpus, content_transformer(removeURL))\n\nremoveCompleteNumbers <- function(x) str_replace_all(x,'(?<=[:blank:])[0-9]+(?=[:blank:])', '')\nTweets_Corpus = tm_map(Tweets_Corpus, content_transformer(removeCompleteNumbers))\n\nTweets_Corpus = tm_map(Tweets_Corpus,removePunctuation)\nremoveSingle <- function(x) gsub(\" . \", \" \", x)   \nTweets_Corpus = tm_map(Tweets_Corpus, content_transformer(removeSingle))\nTweets_Corpus = tm_map(Tweets_Corpus,stripWhitespace)\nTweets_Corpus = tm_map(Tweets_Corpus,content_transformer(tolower))\nTweets_Corpus = tm_map(Tweets_Corpus, content_transformer(replace_abbreviation))\n\nTweets_Corpus_Filtered = Tweets_Corpus\nTweets_Corpus<-tm_map(Tweets_Corpus, stemDocument,language = 'english')\nstemCompletionuserDefined <- function(x,dictionary) {\n  x = unlist(strsplit(as.character(x),\" \"))\n  x = x[x !=\"\"]\n  x = stemCompletion(x, dictionary = dictionary)\n  x = paste(x, sep=\"\", collapse=\" \")\n  PlainTextDocument(stripWhitespace(x))\n}\nTweets_Corpus = lapply(Tweets_Corpus, stemCompletionuserDefined, dictionary=Tweets_Corpus_Filtered)\ntemp = character()\nfor(i in 1: nrow(TT_dataframe)){\n  temp[i] = Tweets_Corpus[[i]]$content\n}\ntemp_df = data.frame(text = temp, stringsAsFactors = F)\nTweets_Corpus = Corpus(VectorSource(temp_df$text))\nTweets_Corpus = tm_map(Tweets_Corpus, \n                       removeWords,c(stopwords('en'),'rt','thursdaythoughts','thursdaythought'))\n```\n\n###4.Word Frequency Analysis\n* Convert the corpus into `TermDocumentMatrix`.\n* Calculate the frequency count of each word and create a dataframe of words & their frequencies.\n```{r results='hide',message=FALSE}\nTweets_Corpus_TDM = TermDocumentMatrix(Tweets_Corpus)\nTweets_Corpus_TDM_M = as.matrix(Tweets_Corpus_TDM)\ntermFrequency = rowSums(Tweets_Corpus_TDM_M)\ntermFrequency = sort(termFrequency,decreasing =T)\n```\n\nFollowing is the chart displaying the most used 25 words and their frequencies.\n```{r}\nfrequency_df <- data.frame(term = names(termFrequency), freq= termFrequency)\nggplot(frequency_df[1:25,], aes(reorder(term, freq),freq)) +\n  theme_bw() + \n  geom_bar(stat = \"identity\") + \n  coord_flip() +\n  labs(list(title=\"Term Frequency Chart\", x=\"Terms\", y=\"Term Counts\")) \n```\n\nFollowing the sample spare matrix representing the words live & people present in the frst 50 tweets\n```{r}\nindex <- which(dimnames(Tweets_Corpus_TDM)$Terms %in% c(\"live\", \"people\"))\nas.matrix(Tweets_Corpus_TDM[index,1:50])\n```\n\nFollowing is the word cloud consisting the most frequently used words\n```{r warning=FALSE}\npal<- brewer.pal(4, \"Dark2\")\nwordcloud(words = names(termFrequency), \n          freq = termFrequency, \n          min.freq = 2, \n          random.order = F, \n          colors = pal, \n          max.words = 250)\n```\n\n###5.Topic Modeling \nHere we found top 5 discussed topics and their words\n```{r warning=FALSE}\nTweets_Corpus_DTM <- as.DocumentTermMatrix(Tweets_Corpus_TDM)\nrowTotals <- apply(Tweets_Corpus_DTM , 1, sum)\nNullDocs <- Tweets_Corpus_DTM[rowTotals==0, ]\nTweets_Corpus_DTM   <- Tweets_Corpus_DTM[rowTotals> 0, ]\nif (length(NullDocs$dimnames$Docs) > 0) {\n  TT_dataframe <- TT_dataframe[-as.numeric(NullDocs$dimnames$Docs),]\n}\nlda <- LDA(Tweets_Corpus_DTM, k = 5) \nterm <- terms(lda, 5) \nterm\ntopics<- topics(lda)\ntopics_df<- data.frame(date=(TT_dataframe$created), topic = topics)\nqplot(date, ..count.., data=topics_df,\n      geom =\"density\",\n       fill= term[topic],position=\"stack\")\n```\n\n###6.Sentiment Analysis \nHere we tried to find the positive and negative sentiment words and various emotional sentiment words\n* Took the common words from `bing` lexicon to get the +ve or -ve sentiment details for each word.\n* Applied `afinn` lexicon on to the above result to get the sentiment scores for each word.\n```{r results='hide', warning=FALSE}\ncolnames(frequency_df) = c('word','freq')\nsentiment_df = frequency_df %>% \n  inner_join(get_sentiments('bing'), by='word')\nsentiment_df = sentiment_df %>%\n  inner_join(get_sentiments('afinn'),by='word') \n```\n\nFollowing is the graph showing the top used words and their +/- Sentiment\n```{r}\nsentiment_df %>%\n  top_n(25,freq) %>%\n  mutate(x1 = reorder(word,freq),\n         y1 = ifelse(sentiment=='positive',freq,-freq)) %>%\n  ggplot(aes(x=x1, y= y1, fill = sentiment)) +\n  geom_col(stat='identity') +\n  coord_flip()+\n  facet_wrap(~sentiment, scales = \"free_x\")+\n  ggtitle('Lexicon based word frequency')+\n  xlab('Words')+ylab('Frequency')\n```\n\nHere we tried the same but with regards to various emotional sentiments\n```{r warning=FALSE}\nEmotion_sentiment_df = frequency_df %>% \n  inner_join(get_sentiments('nrc'), by='word')\nEmotion_sentiment_df %>%\n  group_by(sentiment) %>%\n  top_n(10,freq) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, freq)) %>%\n  ggplot(aes(x = word, y = freq , fill  = sentiment)) +\n  scale_fill_brewer(palette=\"Spectral\")+\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ sentiment, scales = \"free\") +\n  coord_flip()+\n  ggtitle('Emotion based word frequency')+\n  xlab('Words')+ylab('Frequency')\n```\n",
    "created" : 1505586385642.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3822793011",
    "id" : "FDFF7DDB",
    "lastKnownWriteTime" : 1505587120,
    "last_content_update" : 1505587120217,
    "path" : "~/GitHub/Projects/Twitter HashTag Analysis/CokeVsPepsi.Rmd",
    "project_path" : "CokeVsPepsi.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 8,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}