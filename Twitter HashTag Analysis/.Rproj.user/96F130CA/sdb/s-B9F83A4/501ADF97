{
    "collab_server" : "",
    "contents" : "if (Sys.getenv(\"JAVA_HOME\")!=\"\") Sys.setenv(JAVA_HOME=\"\")\n\nrequire(twitteR)\nsetup_twitter_oauth(\n  consumer_key = \"MTfL0uv6oBbtVBa3dPuR4PS51\",\n  consumer_secret = \"T480vrOtvSK1XxGcPp20SdRoW99by3EGVvAapzyq8io0uH6lUh\",\n  access_token = \"191958252-uOisKMkvWHWmiTRQnf9d6XMww5wtZVtox1ShcjMm\",\n  access_secret = \"9kTTYtORX4KnXk9SG4R9bQ1PpmQyBYEwmfvDksTh6Wht0\")\ngetCurRateLimitInfo(resources='search')\n\nThursdayThoughts= searchTwitter(searchString ='#ThursdayThoughts',\n                                n=2500 , lang = 'en') \n\n# last ran at 10:33 PM 15-Sep-2017\nThursdayThoughts_RT_Removed = strip_retweets(ThursdayThoughts,\n                                             strip_manual = T,\n                                             strip_mt = T)\nTT_dataframe = twListToDF(ThursdayThoughts)\n\nPepsiTweets =  searchTwitter(searchString ='Pepsi',\n                       n=10000 , lang = 'en') \n\n\nTT_dataframe = twListToDF(ThursdayThoughts)\nwrite.csv(TT_dataframe,'TT_dataframe.csv')\n\n        TT_dataframe = read.csv(file = 'TT_dataframe.csv', header = T)\n        TT_dataframe = TT_dataframe[-1]\n        TT_dataframe = TT_dataframe[1:200,]\n        TT_dataframe$created[1]\n        TT_dataframe$created <- as.Date(TT_dataframe$created,format = \"%d-%m-%Y %H:%M\")\n        \nTweets = as.character(TT_dataframe$text)\nrequire(tm)\nTweets_Corpus = Corpus(VectorSource(Tweets))\n\nrequire(stringr)\nremoveUsernameInRT <- function(x) str_replace_all(x,\"(?<=@)[^\\\\s:]+\",'')\nTweets_Corpus = tm_map(Tweets_Corpus, content_transformer(removeUsernameInRT))\n\nremoveURL <- function(x) gsub(\"http[^[:space:]]*\", \"\", x)\nTweets_Corpus = tm_map(Tweets_Corpus, content_transformer(removeURL))\n\nremoveCompleteNumbers <- function(x) str_replace_all(x,'(?<=[:blank:])[0-9]+(?=[:blank:])', '')\nTweets_Corpus = tm_map(Tweets_Corpus, content_transformer(removeCompleteNumbers))\n\nTweets_Corpus = tm_map(Tweets_Corpus,removePunctuation)\n#Tweets_Corpus = tm_map(Tweets_Corpus, removeWords,c(stopwords('en'),'RT','ThursdayThoughts'))\n\nremoveSingle <- function(x) gsub(\" . \", \" \", x)   \nTweets_Corpus = tm_map(Tweets_Corpus, content_transformer(removeSingle))\nTweets_Corpus = tm_map(Tweets_Corpus,stripWhitespace)\nTweets_Corpus = tm_map(Tweets_Corpus,content_transformer(tolower))\nrequire(qdap)\nTweets_Corpus = tm_map(Tweets_Corpus, content_transformer(replace_abbreviation))\n\nTweets_Corpus_Filtered = Tweets_Corpus\n# Tweets_Corpus = Tweets_Corpus_Filtered\nTweets_Corpus<-tm_map(Tweets_Corpus, stemDocument,language = 'english')\nstemCompletionuserDefined <- function(x,dictionary) {\n  x = unlist(strsplit(as.character(x),\" \"))\n  x = x[x !=\"\"]\n  x = stemCompletion(x, dictionary = dictionary)\n  x = paste(x, sep=\"\", collapse=\" \")\n  PlainTextDocument(stripWhitespace(x))\n}\nTweets_Corpus = lapply(Tweets_Corpus, stemCompletionuserDefined, dictionary=Tweets_Corpus_Filtered)\ntemp = character()\nfor(i in 1: nrow(TT_dataframe)){\n  temp[i] = Tweets_Corpus[[i]]$content\n}\ntemp_df = data.frame(text = temp, stringsAsFactors = F)\nTweets_Corpus = Corpus(VectorSource(temp_df$text))\nTweets_Corpus = tm_map(Tweets_Corpus, removeWords,c(stopwords('en'),'rt','thursdaythoughts','thursdaythought'))\n\nTweets_Corpus_TDM = TermDocumentMatrix(Tweets_Corpus)\nTweets_Corpus_TDM_M = as.matrix(Tweets_Corpus_TDM)\n\n## Word Frequency Analysis----\ntermFrequency = rowSums(Tweets_Corpus_TDM_M)\ntermFrequency = sort(termFrequency,decreasing =T)\n\n# termFrequency[1:25]\n# barplot(termFrequency[1:25],\n#         col='tan',las=2)\n\nrequire(ggplot2)\nfrequency_df <- data.frame(term = names(termFrequency), freq= termFrequency)\nggplot(frequency_df[1:25,], aes(reorder(term, freq),freq)) +\n  theme_bw() + \n  geom_bar(stat = \"identity\") + \n  coord_flip() +\n  labs(list(title=\"Term Frequency Chart\", x=\"Terms\", y=\"Term Counts\")) \n\n## Word Mention Matrix  ----\nindex <- which(dimnames(Tweets_Corpus_TDM)$Terms %in% c(\"live\", \"people\"))\nas.matrix(Tweets_Corpus_TDM[index,21:60])\n\n## Word Cloud ----\npal<- brewer.pal(4, \"Dark2\")\nrequire(wordcloud)\nwordcloud(words = names(termFrequency), \n          freq = termFrequency, \n          min.freq = 2, \n          random.order = F, \n          colors = pal, \n          max.words = 100)\n## Word Correlation ----\n# WordCorrelation <- apply_as_df(Tweets_Corpus[1:100], word_cor, word = \"save\", r=.01)\n# plot(WordCorrelation)\n\n## Topic Modeling ----\nTweets_Corpus_DTM <- as.DocumentTermMatrix(Tweets_Corpus_TDM)\nrowTotals <- apply(Tweets_Corpus_DTM , 1, sum)\nNullDocs <- Tweets_Corpus_DTM[rowTotals==0, ]\nTweets_Corpus_DTM   <- Tweets_Corpus_DTM[rowTotals> 0, ]\nif (length(NullDocs$dimnames$Docs) > 0) {\n  TT_dataframe <- TT_dataframe[-as.numeric(NullDocs$dimnames$Docs),]\n}\nrequire(topicmodels)\nlda <- LDA(Tweets_Corpus_DTM, k = 5) # find 5 topic\nterm <- terms(lda, 7) # first 7 terms of every topic\nterm\ntopics<- topics(lda)\ntopics\ntopics_df<- data.frame(date=(TT_dataframe$created), topic = topics)\nqplot(date, ..count.., data=topics_df,\n      geom =\"density\",\n       fill= term[topic],position=\"stack\")\n## Sentiment Analysis ----\ncolnames(frequency_df) = c('word','freq')\nrequire(tidytext)\nrequire(dplyr)\nsentiment_df = frequency_df %>% \n  inner_join(get_sentiments('bing'), by='word')\nsentiment_df = sentiment_df %>%\n  inner_join(get_sentiments('afinn'),by='word') \n\nsentiment_df %>%\n  top_n(25,freq) %>%\n  mutate(x1 = reorder(word,freq),\n         y1 = ifelse(sentiment=='positive',freq,-freq)) %>%\n  ggplot(aes(x=x1, y= y1, fill = sentiment)) +\n  geom_col(stat='identity') +\n  coord_flip()+\n  facet_wrap(~sentiment, scales = \"free_x\")+\n  ggtitle('Lexicon based word frequency')+\n  xlab('Words')+ylab('Frequency')\n\nEmotion_sentiment_df = frequency_df %>% \n  inner_join(get_sentiments('nrc'), by='word')\n\nEmotion_sentiment_df %>%\n  group_by(sentiment) %>%\n  top_n(10,freq) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, freq)) %>%\n  ggplot(aes(x = word, y = freq , fill  = sentiment)) +\n  scale_fill_brewer(palette=\"Spectral\")+\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ sentiment, scales = \"free\") +\n  coord_flip()+\n  ggtitle('Emotion based word frequency')+\n  xlab('Words')+ylab('Frequency')\n\n\n\n\n\n",
    "created" : 1505488952748.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4119829659",
    "id" : "501ADF97",
    "lastKnownWriteTime" : 1505576460,
    "last_content_update" : 1505576460776,
    "path" : "~/GitHub/Projects/Twitter HashTag Analysis/ClassWork.R",
    "project_path" : "ClassWork.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}