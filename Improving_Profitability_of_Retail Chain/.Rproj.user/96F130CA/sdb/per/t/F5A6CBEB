{
    "collab_server" : "",
    "contents" : "require(readxl) # Excel-File reader\nrequire(dplyr) # Data Manupulation\nrequire(forecast) # Time Series \nrequire(tidyr) # splits a col into two based on the regular expression\nrequire(car) # to set un-ordered factors ranking method, helmert - baseline is one method rest referring to it\nrequire(dummies) # to reacte dummy variables for factor data\nrequire(ggplot2) # Visualization \nrequire(usdm) # VIF\nrequire(tseries) # ADF test\nrequire(gridExtra) # multi ggplot in one pannel\nrequire(nlme) #Handline autocorrelation for linear regression\n\n# Building the Rawdata frame - complete data ------\nexcel_sheets('dunnhumby - Breakfast at the Frat.xlsx')\nrawdata = read_excel(path = 'dunnhumby - Breakfast at the Frat.xlsx', sheet = 'dh Transaction Data')\nrawStoreData = read_excel(path = 'dunnhumby - Breakfast at the Frat.xlsx', sheet = 'dh Store Lookup')\nrawdata = rawdata %>% \n  left_join(rawStoreData[which(!duplicated(rawStoreData$STORE_ID)),],\n            by = c('STORE_NUM'='STORE_ID')) %>%\n  left_join(read_excel(path = 'dunnhumby - Breakfast at the Frat.xlsx', sheet = 'dh Products Lookup'),\n            by = 'UPC')\nrawdata$WEEK_END_DATE = as.Date(rawdata$WEEK_END_DATE)\n# Checking for missing values - 3 fields have missing data ----\nMissing_data_Check <- function(data_set){\n  NA_Count = sapply(data_set,function(y) sum(length(which(is.na(y))))) \n  Null_Count = sapply(data_set,function(y) sum(length(which(is.null(y)))))\n  Length0_Count = sapply(data_set,function(y) sum(length(which(length(y)==0))))\n  Empty_Count = sapply(data_set,function(y) {if(class(y) != 'Date') \n    return(sum(length(which(y==''))))\n    else return(0)})\n  Total_NonData = NA_Count+Null_Count+Length0_Count+Empty_Count\n  return( Total_NonData )\n}\nMissing_data_Check(rawdata)\n# Missing data handling ----\n# Fixing 1.1 BasePrice - replace with the weekly product average spend\nnon_baseprice_missing_rawdata = rawdata[,1:25][!apply(rawdata[,9], 1, function(x) any(is.na(x))),] \nproduct_WeeklyAvg_basePrice = non_baseprice_missing_rawdata %>%\n  dplyr::select(WEEK_END_DATE,UPC,BASE_PRICE)%>%\n  group_by(UPC,WEEK_END_DATE) %>%\n  mutate(AVG_BASE_PRICE= round(mean(BASE_PRICE),2),\n         MED_BASE_PRICE = round(median(BASE_PRICE),2)) %>%\n  dplyr::select(WEEK_END_DATE,UPC,AVG_BASE_PRICE) %>%\n  unique()\n\nbaseprice_missing_rawdata = rawdata[,1:25][apply(rawdata[,9], 1, function(x) any(is.na(x))),] \nbaseprice_missing_rawdata = baseprice_missing_rawdata %>%\n  left_join(product_WeeklyAvg_basePrice,\n            by=c(\"WEEK_END_DATE\",\"UPC\"))\nbaseprice_missing_rawdata$BASE_PRICE = baseprice_missing_rawdata$AVG_BASE_PRICE\nrawdata_BasePrice = rbind(non_baseprice_missing_rawdata,baseprice_missing_rawdata[-26])\nrm(list = c('non_baseprice_missing_rawdata','product_WeeklyAvg_basePrice','baseprice_missing_rawdata'))\n# Fixing 1.2 Price: replace by the weekly product average spend\nnon_price_missing_rawdata = rawdata_BasePrice[,1:25][!apply(rawdata_BasePrice[,8], 1, function(x) any(is.na(x))),] \nproduct_WeeklyAvg_Price = non_price_missing_rawdata %>%\n  dplyr::select(WEEK_END_DATE,UPC,PRICE)%>%\n  group_by(UPC,WEEK_END_DATE) %>%\n  mutate(AVG_PRICE= round(mean(PRICE),2)) %>%\n  dplyr::select(WEEK_END_DATE,UPC,AVG_PRICE) %>%\n  unique()\nprice_missing_rawdata = rawdata_BasePrice[,1:25][apply(rawdata_BasePrice[,8], 1, function(x) any(is.na(x))),] \nprice_missing_rawdata = price_missing_rawdata %>%\n  left_join(product_WeeklyAvg_Price,\n            by=c(\"WEEK_END_DATE\",\"UPC\"))\nprice_missing_rawdata$PRICE = price_missing_rawdata$AVG_PRICE\nrawdata_BasePrice_Price = rbind(non_price_missing_rawdata,price_missing_rawdata[-26])\nrm(list = c('non_price_missing_rawdata','product_WeeklyAvg_Price','price_missing_rawdata','rawdata_BasePrice'))\n# Creating Discount based features & Modifying Volume units ----\nrawdata_BasePrice_Price$DISCOUNT_PRICE = rawdata_BasePrice_Price$BASE_PRICE - rawdata_BasePrice_Price$PRICE\nrawdata_BasePrice_Price$DISCOUNT_PERCENT = (rawdata_BasePrice_Price$DISCOUNT_PRICE/rawdata_BasePrice_Price$BASE_PRICE)*100\nrawdata_BasePrice_Price$DISCOUNT = ifelse(rawdata_BasePrice_Price$DISCOUNT_PRICE!=0,1,0)\n# re-defining features as factors\nrawdata_factors = as.data.frame(unclass(rawdata_BasePrice_Price))\nrawdata_factors$FEATURE = as.factor(rawdata_factors$FEATURE)\nrawdata_factors$DISPLAY = as.factor(rawdata_factors$DISPLAY)\nrawdata_factors$TPR_ONLY = as.factor(rawdata_factors$TPR_ONLY)\nrawdata_factors$DISCOUNT = as.factor(rawdata_factors$DISCOUNT)\nrawdata_factors$MSA_CODE = as.factor(rawdata_factors$MSA_CODE)\nrm(rawdata_BasePrice_Price)\n# Forecast of the next 12 Weeks overall Spends for the company -----\n# building data as per weekly total spend happened\ncompany_data = rawdata_factors %>%\n                  dplyr::select(WEEK_END_DATE,SPEND,FEATURE,DISPLAY,TPR_ONLY,\n                                BASE_PRICE,PRICE) %>%\n                  group_by(WEEK_END_DATE) %>%\n                  mutate(W_TOTAL_SPEND = sum(SPEND)) %>%\n                  dplyr::select(WEEK_END_DATE,W_TOTAL_SPEND) %>%\n                  unique() %>%\n                  arrange(WEEK_END_DATE)\n# reading spend data as TimeSeries, split and plot \nTS_data = ts(company_data$W_TOTAL_SPEND, start = c(2009,1), end = c(2011,52), frequency = 52)\nTS_data_train = ts(company_data$W_TOTAL_SPEND[1:144], start = c(2009,1), end = c(2011,40), frequency = 52)\nTS_data_test = ts(company_data$W_TOTAL_SPEND[145:156], start = c(2011,41), end = c(2011,52), frequency = 52)\nplot(TS_data_train, xlab= '2009_1W - 2011_40W', ylab='Weekly Total Sales',main = \"Company's Sales Trend - train data\")\n# plotting the decomposed times series\nTS_data_train_decompose = decompose(TS_data_train)\nplot(TS_data_train_decompose) \n# differenced series & autocorrelation\nTS_data_train_D1 = diff(TS_data_train, lag = 1)\nplot(TS_data_train_D1, xlab= '2009_1W - 2011_40W', ylab='',main = \"1st differential - train data\")\nTS_data_train_D2 = diff(TS_data_train, lag = 2)\nplot(TS_data_train_D2, xlab= '2009_1W - 2011_40W', ylab='',main = \"2nd differential - train data\")\n\nTS_data_train_AutoCorr = acf(TS_data_train,lag.max = 50)\nTS_data_train_PartialAutoCorr = pacf(TS_data_train, lag.max = 10)  \n# Augmented Dickey-Fuller test, Ho:a unit root of a univarate time series\nadf.test(TS_data_train/scale(TS_data_train-TS_data_train_decompose$seasonal),\n         alternative = \"stationary\")\n\n# Time series analysis for the next 12 Week sales prediction for the company.\n# Exponential model\nTS_data_train_expSmoothing = ets(TS_data_train)\nTS_data_train_expSmoothing_forecast = forecast(TS_data_train_expSmoothing,h=12)\nAccuracy_expSmoothing = accuracy(TS_data_train_expSmoothing_forecast,TS_data_test)\nplot(TS_data_train_expSmoothing_forecast)\n# Holt-Winters model - optimal values for alpha, beta, gamma\nj=1\nMAPE = numeric()\na1 = numeric()\nb1 = numeric()\nc1 = numeric()\nfor(a in seq(from=0.1,to= 0.9,by=0.1)){\n  for(b in seq(from=0.1,to= 0.9,by=0.1)){\n    for(c in seq(from=0.1,to= 0.9,by=0.1)){\n      HW_Model = HoltWinters(TS_data_train, alpha=a, beta=b, gamma=c)\n      hold_predict = forecast(HW_Model,12)\n      MAPE[j] = sum(abs(hold_predict$mean[1:12]-TS_data_test[1:12])/TS_data_test[1:12])/length(TS_data_test)\n      a1[j] = a\n      b1[j] = b\n      c1[j] =c\n      j = j+1\n    }\n  }\n  print(a)\n}\ntemp = data.frame(MAPE = MAPE, alpha = a1, beta = b1, gamma = c1)\nsubset(temp, MAPE==min(MAPE))\n\nTS_data_train_HW= HoltWinters(TS_data_train, alpha=0.1, beta=0.5, gamma=0.6)\nTS_data_train_HW_forecast= forecast(TS_data_train_HW, 12)\nplot(TS_data_train_HW_forecast)\nAccuracy_HW=accuracy(TS_data_train_HW_forecast,TS_data_test)\n\nBox.test(TS_data_train_HW_forecast$residuals, lag = 20, type = 'Ljung-Box')\n# H0: The data is independently distributed i,e no correlation.\n# H1: The data is not independently distributed i,e Correlation is evident\n\n# ARIMA model\nTS_data_train_ARIMA=auto.arima(TS_data_train)\nTS_data_train_ARIMA_forecast = forecast(TS_data_train_ARIMA, h=12)\nplot(TS_data_train_ARIMA_forecast)\nAccuracy_ARIMA=accuracy(TS_data_train_ARIMA_forecast,TS_data_test)\n\nacf(TS_data_train, lag.max = 20)\npacf(TS_data_train, lag.max = 20)\n#Ho:a unit root , H1: is not a unit root ,i.e Stationary\nadf.test(diff(TS_data_train, lag = 1),alternative = \"stationary\")\n\nBox.test(TS_data_train_ARIMA_forecast$residuals, lag = 20, type = 'Ljung-Box')\n# H0: The data is independently distributed i,e no correlation.\n# H1: The data is not independently distributed i,e Correlation is evident\n\nplotForecastErrors <- function(forecasterrors)\n{\n  # make a histogram of the forecast errors:\n  mybinsize <- IQR(forecasterrors)/4\n  mysd <- sd(forecasterrors)\n  mymin <- min(forecasterrors) - mysd*5\n  mymax <- max(forecasterrors) + mysd*3\n  # generate normally distributed data with mean 0 and standard deviation mysd\n  mynorm <- rnorm(10000, mean=0, sd=mysd)\n  mymin2 <- min(mynorm)\n  mymax2 <- max(mynorm)\n  if (mymin2 < mymin) { mymin <- mymin2 }\n  if (mymax2 > mymax) { mymax <- mymax2 }\n  # make a red histogram of the forecast errors, with the normally distributed data overlaid:\n    mybins <- seq(mymin, mymax, mybinsize)\nhist(forecasterrors, col=\"red\", freq=FALSE, breaks=mybins)\n# freq=FALSE ensures the area under the histogram = 1\n# generate normally distributed data with mean 0 and standard deviation mysd\nmyhist <- hist(mynorm, plot=FALSE, breaks=mybins)\n# plot the normal curve as a blue line on top of the histogram of forecast errors:\n  points(myhist$mids, myhist$density, type=\"l\", col=\"blue\", lwd=2)\n}\nplotForecastErrors(TS_data_train_ARIMA_forecast$residuals)\n\n# TbATS model\nTS_data_train_TBATS = tbats(TS_data_train)\nTS_data_train_TBATS_forecast = forecast(TS_data_train_TBATS, h=12)\nplot(TS_data_train_TBATS_forecast)\nAccuracy_TBATS=accuracy(TS_data_train_TBATS_forecast,TS_data_test)\n# NNETS model      \nTS_data_train_NN = nnetar(TS_data_train)\nTS_data_train_NN_forecast = forecast(TS_data_train_NN, h=12)\nplot(TS_data_train_NN_forecast)\nAccuracy_NN=accuracy(TS_data_train_NN_forecast,TS_data_test)\n\nAccuracy_expSmoothing\nAccuracy_HW\nAccuracy_ARIMA\nAccuracy_TBATS\nAccuracy_NN\n\n# Prediction of sales for next 12 Weeks - ARIMA model\nARIMA_model_completedata=auto.arima(TS_data)\nTS_data_forecast = forecast(ARIMA_model_completedata, h=12)\nplot(TS_data_forecast)\n\n# Hypothesis testing ----\n# 1,2.Store AVG_WEEKLY_BASKETS contributors \nstore_data = rawStoreData\nstore_data$PARKING_AVAIL = ifelse(is.na(store_data$PARKING_SPACE_QTY),0,1)\nstore_data$PARKING_SPACE_QTY = ifelse(is.na(store_data$PARKING_SPACE_QTY),0,store_data$PARKING_SPACE_QTY)\n\nstore_data$ADDRESS_STATE_PROV_CODE = as.factor(store_data$ADDRESS_STATE_PROV_CODE)\nstore_data$SEG_VALUE_NAME = as.factor(store_data$SEG_VALUE_NAME)\nstore_data$ADDRESS_CITY_NAME = as.factor(store_data$ADDRESS_CITY_NAME)\n\ntemp = dummy.data.frame(as.data.frame(store_data[,c(4,6,7,8,9)]),sep='_') #state,seg,parking,store size, bucket size\nusdm::vif(as.data.frame(temp[c(1:3,5,6,8,9)])) # passing 3 states, 2 seg, parking and store size\nusdm::vif(as.data.frame(store_data[,c(7,8)])) # parking, store size and bucket size\n\nStore_St_Se_P_A_model = lm(AVG_WEEKLY_BASKETS~., data=temp[c(1:3,5,6,8:10)] )\nsummary(Store_St_Se_P_A_model)\nStore_P_A = lm(AVG_WEEKLY_BASKETS~., data= store_data[,c(7,8,9)])\nStore_P_A = lm(AVG_WEEKLY_BASKETS~PARKING_SPACE_QTY+SALES_AREA_SIZE_NUM+SALES_AREA_SIZE_NUM*PARKING_SPACE_QTY, data= store_data[,c(7,8,9)])\nsummary(Store_P_A)\n\ncar::durbinWatsonTest(Store_P_A)\n#H0: There is no correlation\n#H1: There is correlation among error terms\nBox.test(Store_P_A$residuals, lag = 1, type = 'Ljung-Box')\n# H0: The data is independently distributed i,e no correlation.\n# H1: The data is not independently distributed i,e Correlation is evident\n\npar(mfrow=c(2,2))\nplot(Store_P_A) \ndev.off()\n\n# Residuals vs Fitted values: No pattern => independent variables are Linear & Additive in nature \n# Normal Q-Q  plot: Stright line => error terms are normally distriuted \n# Scale-Location: point are scattered, no forming a funnel => No Hetero skedasticity\n# Residuals vs Leverage: 58th point is a influencial outlier\nusdm::vif(as.data.frame(store_data[-c(58,27),c(7,8)]))\nStore_P_A1 = lm(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)])\n#Store_P_A1 = lm(AVG_WEEKLY_BASKETS~PARKING_SPACE_QTY+SALES_AREA_SIZE_NUM+SALES_AREA_SIZE_NUM*PARKING_SPACE_QTY, data= store_data[-c(58,27),c(7,8,9)])\n\n\nsummary(Store_P_A1)\ncar::durbinWatsonTest(Store_P_A1)\n#H0: There is no correlation\n#H1: There is correlation among error terms\nBox.test(Store_P_A1$residuals, lag = 1, type = 'Ljung-Box')\n# H0: The data is independently distributed i,e no correlation.\n# H1: The data is not independently distributed i,e Correlation is evident\n\npar(mfrow=c(2,2))\nplot(Store_P_A1) \ndev.off()\n\nresiduals = Store_P_A1$residuals\nparking_independent_var = store_data[-c(58,27),c(7)] \narea_independent_var = store_data[-c(58,27),c(8)] \n\nAutocorrelation = as.data.frame(cbind(residuals,parking_independent_var))\nAutocorrelation = Autocorrelation[order(Autocorrelation$PARKING_SPACE_QTY),]\nplot(Autocorrelation$residuals, Autocorrelation$PARKING_SPACE_QTY)\n\nAutocorrelation = as.data.frame(cbind(residuals,area_independent_var))\nAutocorrelation = Autocorrelation[order(Autocorrelation$SALES_AREA_SIZE_NUM),]\nplot(Autocorrelation$residuals, Autocorrelation$SALES_AREA_SIZE_NUM)\n\nacf(residuals,lag.max = 20)\npacf(residuals,lag.max = 20)\n\nStore_P_A1_GLS = gls(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)], correlation=corARMA(p=1,q=1))\nsummary(Store_P_A1_GLS)\n\n# 3. No of visits and product sales relationship\nWeekly_Visit_data = rawdata_factors %>%\n                        dplyr::select(WEEK_END_DATE,SPEND,VISITS) %>%\n                        dplyr::group_by(WEEK_END_DATE) %>%\n                        dplyr::mutate(W_TOTAL_SPEND = sum(SPEND),\n                                      W_TOTAL_VISIT = sum(VISITS)) %>%\n                        dplyr::select(WEEK_END_DATE,W_TOTAL_SPEND,W_TOTAL_VISIT) %>%\n                        unique()\nVisit_Sales_model = lm(W_TOTAL_SPEND~W_TOTAL_VISIT, data = Weekly_Visit_data)\nsummary(Visit_Sales_model)\nplot(Visit_Sales_model)\n# Promotional activities ----\n# Product wise Significant promotion\nall_product_list = unique(rawdata_factors$UPC)\nProm_Sig_mode_df = data.frame(\n  productCode = numeric(0),\n  Intercept_PValue = numeric(0),FEATURE_PValue = numeric(0),DISPLAY_PValue = numeric(0),TPR_ONLY_PValue = numeric(0),\n  F_Length = numeric(0),D_Length = numeric(0),T_Length = numeric(0)\n  )\nProm_Sig_mode_df_names = names(Prom_Sig_mode_df)\nfor(i in all_product_list){\n  data1=subset(rawdata,UPC==i)\n  data_a=data1[,c(7,10,11,12)]\n  data_a$FEATURE=as.factor(data_a$FEATURE)\n  data_a$DISPLAY=as.factor(data_a$DISPLAY)\n  data_a$TPR_ONLY=as.factor(data_a$TPR_ONLY)\n  model=lm(SPEND~FEATURE+DISPLAY+TPR_ONLY,data = data1)\n  print(summary(model))\n  temp = coef(summary(model))[, \"Pr(>|t|)\"]\n  Prom_Sig_mode_df = rbind(Prom_Sig_mode_df,\n                           data.frame(i,temp[\"(Intercept)\"],\n                                     temp[\"FEATURE\"],temp[\"DISPLAY\"],\n                                     temp[\"TPR_ONLY\"],length(unique(data1$FEATURE)),\n                                     length(unique(data1$DISPLAY)),length(unique(data1$TPR_ONLY))))\n}\ncolnames(Prom_Sig_mode_df) = Prom_Sig_mode_df_names\nProm_Sig_mode_df$F_Sig = ifelse(Prom_Sig_mode_df$FEATURE_PValue<=0.05,'F','')\nProm_Sig_mode_df$D_Sig = ifelse(Prom_Sig_mode_df$DISPLAY_PValue<=0.05,'D','')\nProm_Sig_mode_df$T_Sig = ifelse(Prom_Sig_mode_df$TPR_ONLY_PValue<=0.05,'T','')\nProm_Sig_mode_df=Prom_Sig_mode_df %>%\n                    dplyr::select(c(1,9,10,11)) %>%\n                    dplyr::mutate(Sig_modes = (F_Sig=='F')+(D_Sig=='D')+(T_Sig=='T')) %>%\n                    dplyr::arrange(desc(Sig_modes))\n# top 10 products Promotion Effectiveness\nTop_50sales_products = c('1600027527','3800031838','7192100339','1600027528',\n                       '1600027564','3800031829','3800039118','7192100337',\n                       '1111085350'#,'1111009477','88491201426', - don't have enough data points\n                       )\nProduct_PE_df = data.frame(numeric(),numeric())\nfor(Product in Top_50sales_products){\n  temp = subset(rawdata_factors, UPC==Product)\n  print(Product)\n  temp$FEATURE = as.numeric(as.character(temp$FEATURE))\n  temp$DISPLAY = as.numeric(as.character(temp$DISPLAY))\n  temp$TPR_ONLY = as.numeric(as.character(temp$TPR_ONLY))\n  prod_data = temp %>%\n    dplyr::select(WEEK_END_DATE,SPEND,FEATURE,DISPLAY,TPR_ONLY,\n                  BASE_PRICE,PRICE) %>%\n    dplyr::group_by(WEEK_END_DATE) %>%\n    dplyr::mutate(W_TOTAL_SPEND = sum(SPEND),\n                 W_FEATURE = ifelse(sum(FEATURE)==0,0,1),\n                 W_DISPLAY = ifelse(sum(DISPLAY)==0,0,1),\n                 W_TPR_ONLY = ifelse(sum(FEATURE)==0,0,1),\n                 W_BASE_PRICE = mean(BASE_PRICE),\n                 W_PRICE = mean(PRICE)) %>%\n    dplyr::select(WEEK_END_DATE,W_TOTAL_SPEND,W_FEATURE,\n                  W_DISPLAY,W_TPR_ONLY,W_BASE_PRICE,W_PRICE) %>%\n    unique()%>%\n    dplyr::mutate(W_PROMO = ifelse(W_FEATURE+W_DISPLAY+W_TPR_ONLY == 0,0,1),\n                  W_DISCOUNT = W_BASE_PRICE-W_PRICE )%>%\n    dplyr::arrange(WEEK_END_DATE)\n  TS_data = ts(prod_data$W_TOTAL_SPEND, start = c(2009,1), end = c(2011,52), frequency = 52)\n  #plot(TS_data)\n  Decomposition = decompose(TS_data)\n  #Decomposition$seasonal\n  #plot(Decomposition$seasonal)\n  prod_data$W_Seasonal = as.numeric(Decomposition$seasonal)\n  vif(data.frame(prod_data[,c(2,3,4,5,6,10)]))\n  vif(data.frame(prod_data[,c(2,3,4,6,10)])) # removing TPR_ONLY due to collinearity\n  LR_Model = lm(W_TOTAL_SPEND ~ W_BASE_PRICE+W_FEATURE+W_DISPLAY+W_Seasonal,\n                data = prod_data)\n  summary(LR_Model)\n  Coef = LR_Model$coefficients\n  Base_Line = Coef[1]+Coef[2]*(prod_data$W_BASE_PRICE)+(Coef[5]*prod_data$W_Seasonal)\n  INC = Coef[3]*(prod_data$W_FEATURE)+(Coef[4]*prod_data$W_DISPLAY)\n  prod_data$BASE_LINE = Base_Line\n  prod_data$INCREMENTAL = INC\n  P_Promo = subset(temp, FEATURE == 1 | DISPLAY == 1| TPR_ONLY == 1)\n  P_Promo = P_Promo %>% \n    dplyr::select(WEEK_END_DATE,SPEND) %>%\n    dplyr::group_by(WEEK_END_DATE) %>%\n    dplyr::mutate(W_PROMO_SPEND = sum(SPEND)) %>%\n    dplyr::select(WEEK_END_DATE,W_PROMO_SPEND) %>%\n    unique()\n  prod_data = prod_data %>%\n    left_join(P_Promo, by = 'WEEK_END_DATE')\n  prod_data$P1_PE = prod_data$INCREMENTAL / prod_data$W_PROMO_SPEND *100\n  #View(data.frame(A=prod_data$INCREMENTAL+prod_data$BASE_LINE, B=prod_data$W_TOTAL_SPEND , C= prod_data$W_TOTAL_SPEND-prod_data$INCREMENTAL-prod_data$BASE_LINE))\n  Product_PE_df = rbind(Product_PE_df,data.frame(Product,mean(prod_data$P1_PE,na.rm=T)))\n}\ncolnames(Product_PE_df) = c('UPC','Promo_Effectiveness')\nView(Product_PE_df)\n\n# Cluster analysis based on store data\n# building store based data frame\nstoredata2 = rawdata_factors %>%\n  group_by(STORE_NUM) %>%\n  dplyr::select(STORE_NUM,STORE_NAME,ADDRESS_CITY_NAME,\n                ADDRESS_STATE_PROV_CODE,MSA_CODE,SEG_VALUE_NAME,\n                PARKING_SPACE_QTY,SALES_AREA_SIZE_NUM,AVG_WEEKLY_BASKETS) %>%\n  unique()\nstoredata1 = rawdata_factors %>%\n  group_by(STORE_NUM,WEEK_END_DATE) %>%\n  mutate(T_WEEKLY_SPEND1 = sum(SPEND),\n         T_WEEKLY_UNITS_SOLD1 = sum(UNITS),\n         T_WEEKLY_VISITS1 = sum(VISITS),\n         T_WEEKLY_HHS1 = sum(HHS)) %>%\n  dplyr::select(STORE_NUM,T_WEEKLY_SPEND1,T_WEEKLY_UNITS_SOLD1,\n                T_WEEKLY_VISITS1,T_WEEKLY_HHS1,WEEK_END_DATE) %>%\n  unique() %>%\n  group_by(STORE_NUM) %>%\n  mutate(AVG_WEEKLY_T_SPEND = mean(T_WEEKLY_SPEND1),\n         AVG_WEEKLY_T_UNITS_SOLD = mean(T_WEEKLY_UNITS_SOLD1),\n         AVG_WEEKLY_T_VISITS = mean(T_WEEKLY_VISITS1),\n         AVG_WEEKLY_T_HHS = mean(T_WEEKLY_HHS1)) %>%\n  dplyr::select(STORE_NUM,AVG_WEEKLY_T_SPEND,AVG_WEEKLY_T_UNITS_SOLD,\n                AVG_WEEKLY_T_VISITS,AVG_WEEKLY_T_HHS)  %>%\n  unique()\nstoredata = left_join(storedata1,storedata2,by='STORE_NUM')\nrm(list = c('storedata1','storedata2'))\nstoredata$PARKING_FLAG = as.factor(ifelse(is.na(storedata$PARKING_SPACE_QTY),0,1))\nstoredata$PARKING_NEW = ifelse(is.na(storedata$PARKING_SPACE_QTY),0,storedata$PARKING_SPACE_QTY)\n# Optimal centers for Clustering \nk_max = 10\nC_data = storedata[,c(15,12,13,4)] #PARKING_NEW,SALES_AREA_SIZE_NUM,AVG_WEEKLY_BASKETS,AVG_WEEKLY_T_VISITS\nset.seed(1234)\nwss = sapply(1:k_max, \n             function(k){\n               kmeans(C_data, k, \n                      nstart=10,iter.max = 10 )$tot.withinss\n               })\nwss\nplot(1:k_max, wss,\n     type=\"b\", pch = 19, frame = FALSE, \n     xlab=\"Number of clusters K\",\n     ylab=\"Total within-clusters sum of squares\",\n     main='Elbeow method using wss')\n# Clustering\nset.seed(1234)\nstoredata$C3=kmeans(storedata[,c(15,12,13,4)],centers = 3)$cluster\nrm(C_data)\n# Cluster's dataframe creating\nClusterdata = storedata %>%\n  group_by(C3) %>%\n  mutate(AVG_WEEKLY_C_T_SPEND = mean(AVG_WEEKLY_T_SPEND),\n         AVG_WEEKLY_C_T_UNITS_SOLD = mean(AVG_WEEKLY_T_UNITS_SOLD),\n         AVG_WEEKLY_C_T_VISITS = mean(AVG_WEEKLY_T_VISITS),\n         AVG_WEEKLY_C_T_HHS = mean(AVG_WEEKLY_T_HHS),\n         AVG_WEEKLY_C_BASKETS = mean(AVG_WEEKLY_BASKETS),\n         SALES_AREA_C_SIZE_NUM = mean(SALES_AREA_SIZE_NUM),\n         NUM_STORE_WITH_PARKING = sum(as.numeric(PARKING_FLAG)),\n         NUM_STORES = n()) %>%\n  dplyr::select(C3,AVG_WEEKLY_C_T_SPEND,NUM_STORE_WITH_PARKING,\n                NUM_STORES,AVG_WEEKLY_C_T_HHS,AVG_WEEKLY_C_BASKETS,\n                AVG_WEEKLY_C_T_UNITS_SOLD,AVG_WEEKLY_C_T_VISITS,\n                SALES_AREA_C_SIZE_NUM) %>%\n  unique()\nView(Clusterdata)\n# Cluster visualization\nggplot(storedata)+\n  geom_point(aes(x= SALES_AREA_SIZE_NUM,\n                 y=AVG_WEEKLY_BASKETS,\n                 color=as.factor(C3),\n                 shape = as.factor(PARKING_FLAG),\n                 size = AVG_WEEKLY_T_VISITS)) +\n  labs(title = \"Store Cluster\", x = \"Store Area\", y = \"Average Weekly Baskets\",\n       color = \"Cluster\", shape = \"Parking Availability\", size= \"Average Weekly Visits\")\n# Sales forecast for each product\nNN_Product_list = c('1600027527','3800031838','3800039118')\ni= 1\nplot_list = list()\nfor(Product in NN_Product_list){\n  print(Product)\n  P1= subset(rawdata_factors, UPC== Product )  \n  P1$FEATURE=as.numeric(as.character(P1$FEATURE))\n  P1$DISPLAY=as.numeric(as.character(P1$DISPLAY))\n  P1$TPR_ONLY=as.numeric(as.character(P1$TPR_ONLY))\n  P1_data = P1 %>%\n    dplyr::select(WEEK_END_DATE,SPEND,FEATURE,DISPLAY,TPR_ONLY,\n                  BASE_PRICE,PRICE) %>%\n    group_by(WEEK_END_DATE) %>%\n    mutate(W_TOTAL_SPEND = sum(SPEND),\n           W_FEATURE = ifelse(sum(FEATURE)==0,0,1),\n           W_DISPLAY = ifelse(sum(DISPLAY)==0,0,1),\n           W_TPR_ONLY = ifelse(sum(FEATURE)==0,0,1),\n           W_BASE_PRICE = mean(BASE_PRICE),\n           W_PRICE = mean(PRICE)) %>%\n    dplyr::select(WEEK_END_DATE,W_TOTAL_SPEND,W_FEATURE,\n                  W_DISPLAY,W_TPR_ONLY,W_BASE_PRICE,W_PRICE) %>%\n    unique()%>%\n    mutate(W_PROMO = ifelse(W_FEATURE+W_DISPLAY+W_TPR_ONLY == 0,0,1),\n           W_DISCOUNT = W_BASE_PRICE-W_PRICE )%>%\n    arrange(WEEK_END_DATE)\n  #print(length(P1_data$W_TOTAL_SPEND))\n  TS_data = ts(P1_data$W_TOTAL_SPEND, start = c(2009,1), end = c(2011,52), frequency = 52)\n  TS_data_train = ts(P1_data$W_TOTAL_SPEND[1:117], start = c(2009,1), end = c(2011,17), frequency = 52)\n  TS_data_test = ts(P1_data$W_TOTAL_SPEND[118:156], start = c(2011,18), end = c(2011,52), frequency = 52)\n  #NNETS Model\n  NN_model = nnetar(TS_data)\n  forecast_12w = forecast(NN_model, h=12)\n  print(forecast_12w$mean)\n  y = c(2,3,4,5,6,7,8,9,10,11,12,13)\n  temp = data.frame(Spend = as.numeric(forecast_12w$mean), Week_No= y )\n  p=ggplot(temp,aes(Week_No,Spend))+\n    geom_line()+\n    geom_point()+\n    ggtitle(Product)\n  plot_list[[i]] = p\n  i= i+1\n}\nARIMA_Product_list = c('7192100339',\n  '1600027528','1600027564','3800031829',\n  '7192100337','1111085350',\n  '88491201426','1111009477')\ni= 4\nfor(Product in ARIMA_Product_list){\n  print(Product)\n  P1= subset(rawdata_factors, UPC== Product )  \n  P1$FEATURE=as.numeric(P1$FEATURE)\n  P1$DISPLAY=as.numeric(P1$DISPLAY)\n  P1$TPR_ONLY=as.numeric(P1$TPR_ONLY)\n  P1_data = P1 %>%\n    dplyr::select(WEEK_END_DATE,SPEND,FEATURE,DISPLAY,TPR_ONLY,\n                  BASE_PRICE,PRICE) %>%\n    group_by(WEEK_END_DATE) %>%\n    mutate(W_TOTAL_SPEND = sum(SPEND),\n           W_FEATURE = ifelse(sum(FEATURE)==0,0,1),\n           W_DISPLAY = ifelse(sum(DISPLAY)==0,0,1),\n           W_TPR_ONLY = ifelse(sum(FEATURE)==0,0,1),\n           W_BASE_PRICE = mean(BASE_PRICE),\n           W_PRICE = mean(PRICE)) %>%\n    dplyr::select(WEEK_END_DATE,W_TOTAL_SPEND,W_FEATURE,\n                  W_DISPLAY,W_TPR_ONLY,W_BASE_PRICE,W_PRICE) %>%\n    unique()%>%\n    mutate(W_PROMO = ifelse(W_FEATURE+W_DISPLAY+W_TPR_ONLY == 0,0,1),\n           W_DISCOUNT = W_BASE_PRICE-W_PRICE )%>%\n    arrange(WEEK_END_DATE)\n  #print(length(P1_data$W_TOTAL_SPEND))\n  TS_data = ts(P1_data$W_TOTAL_SPEND, start = c(2009,1), end = c(2011,52), frequency = 52)\n  TS_data_train = ts(P1_data$W_TOTAL_SPEND[1:117], start = c(2009,1), end = c(2011,17), frequency = 52)\n  TS_data_test = ts(P1_data$W_TOTAL_SPEND[118:156], start = c(2011,18), end = c(2011,52), frequency = 52)\n  ARIMA_Full = auto.arima(TS_data)\n  Forecast_12W = forecast::forecast(ARIMA_Full, h=12 )\n  print(Forecast_12W$mean)\n  y = c(2,3,4,5,6,7,8,9,10,11,12,13)\n  temp = data.frame(Spend = as.numeric(forecast_12w$mean),Week_No = y )\n  p=ggplot(temp,aes(Week_No,Spend))+\n    geom_line()+\n    geom_point()+\n    ggtitle(Product)\n  plot_list[[i]] = p\n  i= i+1\n}\ngrid.arrange(plot_list[[1]], plot_list[[2]],plot_list[[4]],plot_list[[5]],\n             ncol = 2, nrow = 2)\ngrid.arrange( plot_list[[6]], plot_list[[7]], plot_list[[3]],plot_list[[8]],\n              ncol = 2, nrow = 2)\ngrid.arrange(plot_list[[9]], plot_list[[10]], plot_list[[11]],\n             ncol = 1, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1510601077186.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "880792007",
    "id" : "F5A6CBEB",
    "lastKnownWriteTime" : 1512769050,
    "last_content_update" : 1512769050355,
    "path" : "~/GitHub/Projects/Improving_Profitability_of_Retail Chain/Complete Analysis.R",
    "project_path" : "Complete Analysis.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}