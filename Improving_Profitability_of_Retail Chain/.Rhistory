mynorm <- rnorm(10000, mean=0, sd=mysd)
mymin2 <- min(mynorm)
mymax2 <- max(mynorm)
if (mymin2 < mymin) { mymin <- mymin2 }
if (mymax2 > mymax) { mymax <- mymax2 }
# make a red histogram of the forecast errors, with the normally distributed data overlaid:
mybins <- seq(mymin, mymax, mybinsize)
hist(forecasterrors, col="red", freq=FALSE, breaks=mybins)
# freq=FALSE ensures the area under the histogram = 1
# generate normally distributed data with mean 0 and standard deviation mysd
myhist <- hist(mynorm, plot=FALSE, breaks=mybins)
# plot the normal curve as a blue line on top of the histogram of forecast errors:
points(myhist$mids, myhist$density, type="l", col="blue", lwd=2)
}
plotForecastErrors(TS_data_train_ARIMA_forecast$residuals)
usdm::vif(as.data.frame(temp[c(1:3,5,6,8,9)])) # passing 3 states, 2 seg, parking and store size
store_data = rawStoreData
store_data$PARKING_AVAIL = ifelse(is.na(store_data$PARKING_SPACE_QTY),0,1)
store_data$PARKING_SPACE_QTY = ifelse(is.na(store_data$PARKING_SPACE_QTY),0,store_data$PARKING_SPACE_QTY)
store_data$ADDRESS_STATE_PROV_CODE = as.factor(store_data$ADDRESS_STATE_PROV_CODE)
store_data$SEG_VALUE_NAME = as.factor(store_data$SEG_VALUE_NAME)
store_data$ADDRESS_CITY_NAME = as.factor(store_data$ADDRESS_CITY_NAME)
temp = dummy.data.frame(as.data.frame(store_data[,c(4,6,7,8,9)]),sep='_') #state,seg,parking,store size, bucket size
usdm::vif(as.data.frame(temp[c(1:3,5,6,8,9)])) # passing 3 states, 2 seg, parking and store size
usdm::vif(as.data.frame(store_data[,c(7,8)])) # parking, store size and bucket size
Store_St_Se_P_A_model = lm(AVG_WEEKLY_BASKETS~., data=temp[c(1:3,5,6,8:10)] )
summary(Store_St_Se_P_A_model)
Store_P_A = lm(AVG_WEEKLY_BASKETS~., data= store_data[,c(7,8,9)])
summary(Store_P_A)
summary(Store_P_A)
summary(Store_P_A)
summary(Store_P_A)
summary(Store_P_A)
car::durbinWatsonTest(Store_P_A)
#H0: There is no correlation
#H1: There is correlation among error terms
plot(Store_P_A)
par(mfrow=c(2,2))
plot(Store_P_A)
dev.off()
usdm::vif(as.data.frame(store_data[-c(58,27),c(7,8)]))
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)])
summary(Store_P_A1)
par(mfrow=c(2,2))
plot(Store_P_A1)
dev.off()
par(mfrow=c(2,2))
plot(Store_P_A1)
dev.off()
length(unique(rawdata$STORE_NUM))
length(unique(rawdata$UPC))
length(unique(rawdata$ADDRESS_CITY_NAME))
length(unique(rawdata$MSA_CODE))
length(unique(rawdata$UPC))
length(unique(rawdata$SUB_CATEGORY))
length(unique(rawdata$CATEGORY))
length(unique(rawdata$MANUFACTURER))
?adf.test
Weekly_Visit_data = rawdata_factors %>%
dplyr::select(WEEK_END_DATE,SPEND,VISITS) %>%
dplyr::group_by(WEEK_END_DATE) %>%
dplyr::mutate(W_TOTAL_SPEND = sum(SPEND),
W_TOTAL_VISIT = sum(VISITS)) %>%
dplyr::select(WEEK_END_DATE,W_TOTAL_SPEND,W_TOTAL_VISIT) %>%
unique()
Visit_Sales_model = lm(W_TOTAL_SPEND~W_TOTAL_VISIT, data = Weekly_Visit_data)
summary(Visit_Sales_model)
NN_Product_list = c('1600027527','3800031838','3800039118')
i= 1
plot_list = list()
for(Product in NN_Product_list){
print(Product)
P1= subset(rawdata_factors, UPC== Product )
P1$FEATURE=as.numeric(as.character(P1$FEATURE))
P1$DISPLAY=as.numeric(as.character(P1$DISPLAY))
P1$TPR_ONLY=as.numeric(as.character(P1$TPR_ONLY))
P1_data = P1 %>%
dplyr::select(WEEK_END_DATE,SPEND,FEATURE,DISPLAY,TPR_ONLY,
BASE_PRICE,PRICE) %>%
group_by(WEEK_END_DATE) %>%
mutate(W_TOTAL_SPEND = sum(SPEND),
W_FEATURE = ifelse(sum(FEATURE)==0,0,1),
W_DISPLAY = ifelse(sum(DISPLAY)==0,0,1),
W_TPR_ONLY = ifelse(sum(FEATURE)==0,0,1),
W_BASE_PRICE = mean(BASE_PRICE),
W_PRICE = mean(PRICE)) %>%
dplyr::select(WEEK_END_DATE,W_TOTAL_SPEND,W_FEATURE,
W_DISPLAY,W_TPR_ONLY,W_BASE_PRICE,W_PRICE) %>%
unique()%>%
mutate(W_PROMO = ifelse(W_FEATURE+W_DISPLAY+W_TPR_ONLY == 0,0,1),
W_DISCOUNT = W_BASE_PRICE-W_PRICE )%>%
arrange(WEEK_END_DATE)
#print(length(P1_data$W_TOTAL_SPEND))
TS_data = ts(P1_data$W_TOTAL_SPEND, start = c(2009,1), end = c(2011,52), frequency = 52)
TS_data_train = ts(P1_data$W_TOTAL_SPEND[1:117], start = c(2009,1), end = c(2011,17), frequency = 52)
TS_data_test = ts(P1_data$W_TOTAL_SPEND[118:156], start = c(2011,18), end = c(2011,52), frequency = 52)
#NNETS Model
NN_model = nnetar(TS_data)
forecast_12w = forecast(NN_model, h=12)
print(forecast_12w$mean)
y = c(2,3,4,5,6,7,8,9,10,11,12,13)
temp = data.frame(Spend = as.numeric(forecast_12w$mean), Week_No= y )
p=ggplot(temp,aes(Week_No,Spend))+
geom_line()+
geom_point()+
ggtitle(Product)
plot_list[[i]] = p
i= i+1
}
ARIMA_Product_list = c('7192100339',
'1600027528','1600027564','3800031829',
'7192100337','1111085350',
'88491201426','1111009477')
i= 4
for(Product in ARIMA_Product_list){
print(Product)
P1= subset(rawdata_factors, UPC== Product )
P1$FEATURE=as.numeric(P1$FEATURE)
P1$DISPLAY=as.numeric(P1$DISPLAY)
P1$TPR_ONLY=as.numeric(P1$TPR_ONLY)
P1_data = P1 %>%
dplyr::select(WEEK_END_DATE,SPEND,FEATURE,DISPLAY,TPR_ONLY,
BASE_PRICE,PRICE) %>%
group_by(WEEK_END_DATE) %>%
mutate(W_TOTAL_SPEND = sum(SPEND),
W_FEATURE = ifelse(sum(FEATURE)==0,0,1),
W_DISPLAY = ifelse(sum(DISPLAY)==0,0,1),
W_TPR_ONLY = ifelse(sum(FEATURE)==0,0,1),
W_BASE_PRICE = mean(BASE_PRICE),
W_PRICE = mean(PRICE)) %>%
dplyr::select(WEEK_END_DATE,W_TOTAL_SPEND,W_FEATURE,
W_DISPLAY,W_TPR_ONLY,W_BASE_PRICE,W_PRICE) %>%
unique()%>%
mutate(W_PROMO = ifelse(W_FEATURE+W_DISPLAY+W_TPR_ONLY == 0,0,1),
W_DISCOUNT = W_BASE_PRICE-W_PRICE )%>%
arrange(WEEK_END_DATE)
#print(length(P1_data$W_TOTAL_SPEND))
TS_data = ts(P1_data$W_TOTAL_SPEND, start = c(2009,1), end = c(2011,52), frequency = 52)
TS_data_train = ts(P1_data$W_TOTAL_SPEND[1:117], start = c(2009,1), end = c(2011,17), frequency = 52)
TS_data_test = ts(P1_data$W_TOTAL_SPEND[118:156], start = c(2011,18), end = c(2011,52), frequency = 52)
ARIMA_Full = auto.arima(TS_data)
Forecast_12W = forecast::forecast(ARIMA_Full, h=12 )
print(Forecast_12W$mean)
y = c(2,3,4,5,6,7,8,9,10,11,12,13)
temp = data.frame(Spend = as.numeric(forecast_12w$mean),Week_No = y )
p=ggplot(temp,aes(Week_No,Spend))+
geom_line()+
geom_point()+
ggtitle(Product)
plot_list[[i]] = p
i= i+1
}
grid.arrange(plot_list[[1]], plot_list[[2]],plot_list[[4]],plot_list[[5]],
ncol = 2, nrow = 2)
grid.arrange( plot_list[[6]], plot_list[[7]], plot_list[[3]],plot_list[[8]],
ncol = 2, nrow = 2)
grid.arrange(plot_list[[9]], plot_list[[10]], plot_list[[11]],
ncol = 1, nrow = 3)
grid.arrange(plot_list[[1]], plot_list[[2]],plot_list[[4]],plot_list[[8]],
ncol = 2, nrow = 2)
car::durbinWatsonTest(Store_P_A1)
#H0: There is no correlation
#H1: There is correlation among error terms
Store_St_Se_P_A_model = lm(AVG_WEEKLY_BASKETS~., data=temp[c(1:3,5,6,8:10)] )
store_data = rawStoreData
store_data$PARKING_AVAIL = ifelse(is.na(store_data$PARKING_SPACE_QTY),0,1)
store_data$PARKING_SPACE_QTY = ifelse(is.na(store_data$PARKING_SPACE_QTY),0,store_data$PARKING_SPACE_QTY)
store_data$ADDRESS_STATE_PROV_CODE = as.factor(store_data$ADDRESS_STATE_PROV_CODE)
store_data$SEG_VALUE_NAME = as.factor(store_data$SEG_VALUE_NAME)
store_data$ADDRESS_CITY_NAME = as.factor(store_data$ADDRESS_CITY_NAME)
temp = dummy.data.frame(as.data.frame(store_data[,c(4,6,7,8,9)]),sep='_') #state,seg,parking,store size, bucket size
usdm::vif(as.data.frame(temp[c(1:3,5,6,8,9)])) # passing 3 states, 2 seg, parking and store size
usdm::vif(as.data.frame(store_data[,c(7,8)])) # parking, store size and bucket size
Store_St_Se_P_A_model = lm(AVG_WEEKLY_BASKETS~., data=temp[c(1:3,5,6,8:10)] )
summary(Store_St_Se_P_A_model)
Store_P_A = lm(AVG_WEEKLY_BASKETS~., data= store_data[,c(7,8,9)])
summary(Store_P_A)
mean(store_data$AVG_WEEKLY_BASKETS)
?adf.test
store_data = rawStoreData
store_data$PARKING_AVAIL = ifelse(is.na(store_data$PARKING_SPACE_QTY),0,1)
store_data$PARKING_SPACE_QTY = ifelse(is.na(store_data$PARKING_SPACE_QTY),0,store_data$PARKING_SPACE_QTY)
store_data$ADDRESS_STATE_PROV_CODE = as.factor(store_data$ADDRESS_STATE_PROV_CODE)
store_data$SEG_VALUE_NAME = as.factor(store_data$SEG_VALUE_NAME)
store_data$ADDRESS_CITY_NAME = as.factor(store_data$ADDRESS_CITY_NAME)
temp = dummy.data.frame(as.data.frame(store_data[,c(4,6,7,8,9)]),sep='_') #state,seg,parking,store size, bucket size
usdm::vif(as.data.frame(temp[c(1:3,5,6,8,9)])) # passing 3 states, 2 seg, parking and store size
usdm::vif(as.data.frame(store_data[,c(7,8)])) # parking, store size and bucket size
Store_St_Se_P_A_model = lm(AVG_WEEKLY_BASKETS~., data=temp[c(1:3,5,6,8:10)] )
summary(Store_St_Se_P_A_model)
Store_P_A = lm(AVG_WEEKLY_BASKETS~., data= store_data[,c(7,8,9)])
summary(Store_P_A)
car::durbinWatsonTest(Store_P_A)
?car::durbinWatsonTest
car::durbinWatsonTest(Store_P_A$residuals)
usdm::vif(as.data.frame(store_data[-c(58,27),c(7,8)]))
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)])
summary(Store_P_A1)
car::durbinWatsonTest(Store_P_A1)
car::durbinWatsonTest(Store_P_A)
Box.test(Store_P_A$residuals, lag = 20, type = 'Ljung-Box')
Box.test(Store_P_A$residuals, lag = 1, type = 'Ljung-Box')
car::durbinWatsonTest(Store_P_A)
Box.test(Store_P_A$residuals, lag = 1, type = 'Ljung-Box')
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)])
summary(Store_P_A1)
car::durbinWatsonTest(Store_P_A1)
Box.test(Store_P_A1$residuals, lag = 1, type = 'Ljung-Box')
Box.test(Store_P_A$residuals, lag = 1, type = 'Ljung-Box')
?kmeans
plot(1:k_max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares",
main='Elbeow method using wss')
wss
usdm::vif(as.data.frame(store_data[-c(58),c(7,8)]))
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~., data= store_data[-c(58),c(7,8,9)])
summary(Store_P_A1)
car::durbinWatsonTest(Store_P_A1)
store_data[-c(58,27),c(7)]
Autocorrelation = as.data.frame(cbind(residuals,parking_independent_var))
residuals = Store_P_A1$residuals
parking_independent_var = store_data[-c(58,27),c(7)]
Autocorrelation = as.data.frame(cbind(residuals,parking_independent_var))
length(residuals)
dim(store_data)
usdm::vif(as.data.frame(store_data[-c(58,27),c(7,8)]))
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)])
summary(Store_P_A1)
car::durbinWatsonTest(Store_P_A1)
Box.test(Store_P_A1$residuals, lag = 1, type = 'Ljung-Box')
par(mfrow=c(2,2))
plot(Store_P_A1)
dev.off()
residuals = Store_P_A1$residuals
parking_independent_var = store_data[-c(58,27),c(7)]
Autocorrelation = as.data.frame(cbind(residuals,parking_independent_var))
View(Autocorrelation)
Autocorrelation[order(parking_independent_var,),]
Autocorrelation[order(parking_independent_var),]
Autocorrelation[order(Autocorrelation$parking_independent_var),]
Autocorrelation[order(Autocorrelation$PARKING_SPACE_QTY),]
Autocorrelation = Autocorrelation[order(Autocorrelation$PARKING_SPACE_QTY),]
Plot(Autocorrelation$residuals, Autocorrelation$PARKING_SPACE_QTY)
plot(Autocorrelation$residuals, Autocorrelation$PARKING_SPACE_QTY)
store_data[-c(58,27),c(8)]
area_independent_var = store_data[-c(58,27),c(8)]
Autocorrelation = as.data.frame(cbind(residuals,area_independent_var))
Autocorrelation = as.data.frame(cbind(residuals,area_independent_var))
Autocorrelation = Autocorrelation[order(Autocorrelation$SALES_AREA_SIZE_NUM),]
plot(Autocorrelation$residuals, Autocorrelation$SALES_AREA_SIZE_NUM)
install.packages('nlme')
install.packages("nlme")
require(nlme)
install.packages('nlme')
require(nlme)
require(readxl) # Excel-File reader
require(dplyr) # Data Manupulation
require(forecast) # Time Series
require(tidyr) # splits a col into two based on the regular expression
require(dummies) # to reacte dummy variables for factor data
require(car) # to set un-ordered factors ranking method, helmert - baseline is one method rest referring to it
require(ggplot2) # Visualization
require(usdm) # VIF
require(tseries) # ADF test
require(gridExtra) # multi ggplot in one pannel
require(nlme) #Handline autocorrelation for linear regression
excel_sheets('dunnhumby - Breakfast at the Frat.xlsx')
rawdata = read_excel(path = 'dunnhumby - Breakfast at the Frat.xlsx', sheet = 'dh Transaction Data')
rawStoreData = read_excel(path = 'dunnhumby - Breakfast at the Frat.xlsx', sheet = 'dh Store Lookup')
rawdata = rawdata %>%
left_join(rawStoreData[which(!duplicated(rawStoreData$STORE_ID)),],
by = c('STORE_NUM'='STORE_ID')) %>%
left_join(read_excel(path = 'dunnhumby - Breakfast at the Frat.xlsx', sheet = 'dh Products Lookup'),
by = 'UPC')
rawdata$WEEK_END_DATE = as.Date(rawdata$WEEK_END_DATE)
Missing_data_Check <- function(data_set){
NA_Count = sapply(data_set,function(y) sum(length(which(is.na(y)))))
Null_Count = sapply(data_set,function(y) sum(length(which(is.null(y)))))
Length0_Count = sapply(data_set,function(y) sum(length(which(length(y)==0))))
Empty_Count = sapply(data_set,function(y) {if(class(y) != 'Date')
return(sum(length(which(y==''))))
else return(0)})
Total_NonData = NA_Count+Null_Count+Length0_Count+Empty_Count
return( Total_NonData )
}
Missing_data_Check(rawdata)
non_baseprice_missing_rawdata = rawdata[,1:25][!apply(rawdata[,9], 1, function(x) any(is.na(x))),]
product_WeeklyAvg_basePrice = non_baseprice_missing_rawdata %>%
dplyr::select(WEEK_END_DATE,UPC,BASE_PRICE)%>%
group_by(UPC,WEEK_END_DATE) %>%
mutate(AVG_BASE_PRICE= round(mean(BASE_PRICE),2),
MED_BASE_PRICE = round(median(BASE_PRICE),2)) %>%
dplyr::select(WEEK_END_DATE,UPC,AVG_BASE_PRICE) %>%
unique()
baseprice_missing_rawdata = rawdata[,1:25][apply(rawdata[,9], 1, function(x) any(is.na(x))),]
baseprice_missing_rawdata = baseprice_missing_rawdata %>%
left_join(product_WeeklyAvg_basePrice,
by=c("WEEK_END_DATE","UPC"))
baseprice_missing_rawdata$BASE_PRICE = baseprice_missing_rawdata$AVG_BASE_PRICE
rawdata_BasePrice = rbind(non_baseprice_missing_rawdata,baseprice_missing_rawdata[-26])
rm(list = c('non_baseprice_missing_rawdata','product_WeeklyAvg_basePrice','baseprice_missing_rawdata'))
non_price_missing_rawdata = rawdata_BasePrice[,1:25][!apply(rawdata_BasePrice[,8], 1, function(x) any(is.na(x))),]
product_WeeklyAvg_Price = non_price_missing_rawdata %>%
dplyr::select(WEEK_END_DATE,UPC,PRICE)%>%
group_by(UPC,WEEK_END_DATE) %>%
mutate(AVG_PRICE= round(mean(PRICE),2)) %>%
dplyr::select(WEEK_END_DATE,UPC,AVG_PRICE) %>%
unique()
price_missing_rawdata = rawdata_BasePrice[,1:25][apply(rawdata_BasePrice[,8], 1, function(x) any(is.na(x))),]
price_missing_rawdata = price_missing_rawdata %>%
left_join(product_WeeklyAvg_Price,
by=c("WEEK_END_DATE","UPC"))
price_missing_rawdata$PRICE = price_missing_rawdata$AVG_PRICE
rawdata_BasePrice_Price = rbind(non_price_missing_rawdata,price_missing_rawdata[-26])
rm(list = c('non_price_missing_rawdata','product_WeeklyAvg_Price','price_missing_rawdata','rawdata_BasePrice'))
rawdata_BasePrice_Price$DISCOUNT_PRICE = rawdata_BasePrice_Price$BASE_PRICE - rawdata_BasePrice_Price$PRICE
rawdata_BasePrice_Price$DISCOUNT_PERCENT = (rawdata_BasePrice_Price$DISCOUNT_PRICE/rawdata_BasePrice_Price$BASE_PRICE)*100
rawdata_BasePrice_Price$DISCOUNT = ifelse(rawdata_BasePrice_Price$DISCOUNT_PRICE!=0,1,0)
rawdata_factors = as.data.frame(unclass(rawdata_BasePrice_Price))
rawdata_factors$FEATURE = as.factor(rawdata_factors$FEATURE)
rawdata_factors$DISPLAY = as.factor(rawdata_factors$DISPLAY)
rawdata_factors$TPR_ONLY = as.factor(rawdata_factors$TPR_ONLY)
rawdata_factors$DISCOUNT = as.factor(rawdata_factors$DISCOUNT)
rawdata_factors$MSA_CODE = as.factor(rawdata_factors$MSA_CODE)
rm(rawdata_BasePrice_Price)
company_data = rawdata_factors %>%
dplyr::select(WEEK_END_DATE,SPEND,FEATURE,DISPLAY,TPR_ONLY,
BASE_PRICE,PRICE) %>%
group_by(WEEK_END_DATE) %>%
mutate(W_TOTAL_SPEND = sum(SPEND)) %>%
dplyr::select(WEEK_END_DATE,W_TOTAL_SPEND) %>%
unique() %>%
arrange(WEEK_END_DATE)
store_data = rawStoreData
store_data$PARKING_AVAIL = ifelse(is.na(store_data$PARKING_SPACE_QTY),0,1)
store_data$PARKING_SPACE_QTY = ifelse(is.na(store_data$PARKING_SPACE_QTY),0,store_data$PARKING_SPACE_QTY)
store_data$ADDRESS_STATE_PROV_CODE = as.factor(store_data$ADDRESS_STATE_PROV_CODE)
store_data$SEG_VALUE_NAME = as.factor(store_data$SEG_VALUE_NAME)
store_data$ADDRESS_CITY_NAME = as.factor(store_data$ADDRESS_CITY_NAME)
temp = dummy.data.frame(as.data.frame(store_data[,c(4,6,7,8,9)]),sep='_') #state,seg,parking,store size, bucket size
usdm::vif(as.data.frame(temp[c(1:3,5,6,8,9)])) # passing 3 states, 2 seg, parking and store size
usdm::vif(as.data.frame(store_data[,c(7,8)])) # parking, store size and bucket size
Store_St_Se_P_A_model = lm(AVG_WEEKLY_BASKETS~., data=temp[c(1:3,5,6,8:10)] )
summary(Store_St_Se_P_A_model)
Store_P_A = lm(AVG_WEEKLY_BASKETS~., data= store_data[,c(7,8,9)])
summary(Store_P_A)
car::durbinWatsonTest(Store_P_A)
Box.test(Store_P_A$residuals, lag = 1, type = 'Ljung-Box')
usdm::vif(as.data.frame(store_data[-c(58,27),c(7,8)]))
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)])
summary(Store_P_A1)
car::durbinWatsonTest(Store_P_A1)
Box.test(Store_P_A1$residuals, lag = 1, type = 'Ljung-Box')
acf(residuals,lag.max = 20)
pacf(residuals,lag.max = 20)
?auto.arima
?gls
gls(Store_P_A1, correlation=corARMA(p=1,q=1))
gls(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)], correlation=corARMA(p=1,q=1))
summary(Store_P_A1)
?gls
gls(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)], correlation=corARMA(p=1,q=1))
Store_P_A1_GLS = gls(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)], correlation=corARMA(p=1,q=1))
summary(Store_P_A1_GLS)
summary(Store_P_A1)
length(unique(rawdata_factors$UPC))
names(store_data[-c(58,27),c(7,8,9)])
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~PARKING_SPACE_QTY+SALES_AREA_SIZE_NUM+SALES_AREA_SIZE_NUM*PARKING_SPACE_QTY, data= store_data[-c(58,27),c(7,8,9)])
summary(Store_P_A1)
summary(Store_P_A1)
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)])
par(mfrow=c(2,2))
plot(Store_P_A1)
dev.off()
par(mfrow=c(2,2))
plot(Store_P_A1)
car::durbinWatsonTest(Store_P_A1)
?nnetar
Top_50sales_products = c('1600027527','3800031838','7192100339','1600027528',
'1600027564','3800031829','3800039118','7192100337',
'1111085350'#,'1111009477','88491201426', - don't have enough data points
)
Product_PE_df = data.frame(numeric(),numeric())
all_product_list = unique(rawdata_factors$UPC)
Prom_Sig_mode_df = data.frame(
productCode = numeric(0),
Intercept_PValue = numeric(0),FEATURE_PValue = numeric(0),DISPLAY_PValue = numeric(0),TPR_ONLY_PValue = numeric(0),
F_Length = numeric(0),D_Length = numeric(0),T_Length = numeric(0)
)
Prom_Sig_mode_df_names = names(Prom_Sig_mode_df)
for(i in all_product_list){
data1=subset(rawdata,UPC==i)
data_a=data1[,c(7,10,11,12)]
data_a$FEATURE=as.factor(data_a$FEATURE)
data_a$DISPLAY=as.factor(data_a$DISPLAY)
data_a$TPR_ONLY=as.factor(data_a$TPR_ONLY)
model=lm(SPEND~FEATURE+DISPLAY+TPR_ONLY,data = data1)
print(summary(model))
temp = coef(summary(model))[, "Pr(>|t|)"]
Prom_Sig_mode_df = rbind(Prom_Sig_mode_df,
data.frame(i,temp["(Intercept)"],
temp["FEATURE"],temp["DISPLAY"],
temp["TPR_ONLY"],length(unique(data1$FEATURE)),
length(unique(data1$DISPLAY)),length(unique(data1$TPR_ONLY))))
}
colnames(Prom_Sig_mode_df) = Prom_Sig_mode_df_names
Prom_Sig_mode_df$F_Sig = ifelse(Prom_Sig_mode_df$FEATURE_PValue<=0.05,'F','')
Prom_Sig_mode_df$D_Sig = ifelse(Prom_Sig_mode_df$DISPLAY_PValue<=0.05,'D','')
Prom_Sig_mode_df$T_Sig = ifelse(Prom_Sig_mode_df$TPR_ONLY_PValue<=0.05,'T','')
Prom_Sig_mode_df=Prom_Sig_mode_df %>%
dplyr::select(c(1,9,10,11)) %>%
dplyr::mutate(Sig_modes = (F_Sig=='F')+(D_Sig=='D')+(T_Sig=='T')) %>%
dplyr::arrange(desc(Sig_modes))
Top_50sales_products = c('1600027527','3800031838','7192100339','1600027528',
'1600027564','3800031829','3800039118','7192100337',
'1111085350'#,'1111009477','88491201426', - don't have enough data points
)
Product_PE_df = data.frame(numeric(),numeric())
for(Product in Top_50sales_products){
temp = subset(rawdata_factors, UPC==Product)
print(Product)
temp$FEATURE = as.numeric(as.character(temp$FEATURE))
temp$DISPLAY = as.numeric(as.character(temp$DISPLAY))
temp$TPR_ONLY = as.numeric(as.character(temp$TPR_ONLY))
prod_data = temp %>%
dplyr::select(WEEK_END_DATE,SPEND,FEATURE,DISPLAY,TPR_ONLY,
BASE_PRICE,PRICE) %>%
dplyr::group_by(WEEK_END_DATE) %>%
dplyr::mutate(W_TOTAL_SPEND = sum(SPEND),
W_FEATURE = ifelse(sum(FEATURE)==0,0,1),
W_DISPLAY = ifelse(sum(DISPLAY)==0,0,1),
W_TPR_ONLY = ifelse(sum(FEATURE)==0,0,1),
W_BASE_PRICE = mean(BASE_PRICE),
W_PRICE = mean(PRICE)) %>%
dplyr::select(WEEK_END_DATE,W_TOTAL_SPEND,W_FEATURE,
W_DISPLAY,W_TPR_ONLY,W_BASE_PRICE,W_PRICE) %>%
unique()%>%
dplyr::mutate(W_PROMO = ifelse(W_FEATURE+W_DISPLAY+W_TPR_ONLY == 0,0,1),
W_DISCOUNT = W_BASE_PRICE-W_PRICE )%>%
dplyr::arrange(WEEK_END_DATE)
TS_data = ts(prod_data$W_TOTAL_SPEND, start = c(2009,1), end = c(2011,52), frequency = 52)
#plot(TS_data)
Decomposition = decompose(TS_data)
#Decomposition$seasonal
#plot(Decomposition$seasonal)
prod_data$W_Seasonal = as.numeric(Decomposition$seasonal)
vif(data.frame(prod_data[,c(2,3,4,5,6,10)]))
vif(data.frame(prod_data[,c(2,3,4,6,10)])) # removing TPR_ONLY due to collinearity
LR_Model = lm(W_TOTAL_SPEND ~ W_BASE_PRICE+W_FEATURE+W_DISPLAY+W_Seasonal,
data = prod_data)
summary(LR_Model)
Coef = LR_Model$coefficients
Base_Line = Coef[1]+Coef[2]*(prod_data$W_BASE_PRICE)+(Coef[5]*prod_data$W_Seasonal)
INC = Coef[3]*(prod_data$W_FEATURE)+(Coef[4]*prod_data$W_DISPLAY)
prod_data$BASE_LINE = Base_Line
prod_data$INCREMENTAL = INC
P_Promo = subset(temp, FEATURE == 1 | DISPLAY == 1| TPR_ONLY == 1)
P_Promo = P_Promo %>%
dplyr::select(WEEK_END_DATE,SPEND) %>%
dplyr::group_by(WEEK_END_DATE) %>%
dplyr::mutate(W_PROMO_SPEND = sum(SPEND)) %>%
dplyr::select(WEEK_END_DATE,W_PROMO_SPEND) %>%
unique()
prod_data = prod_data %>%
left_join(P_Promo, by = 'WEEK_END_DATE')
prod_data$P1_PE = prod_data$INCREMENTAL / prod_data$W_PROMO_SPEND *100
#View(data.frame(A=prod_data$INCREMENTAL+prod_data$BASE_LINE, B=prod_data$W_TOTAL_SPEND , C= prod_data$W_TOTAL_SPEND-prod_data$INCREMENTAL-prod_data$BASE_LINE))
Product_PE_df = rbind(Product_PE_df,data.frame(Product,mean(prod_data$P1_PE,na.rm=T)))
}
summary(LR_Model)
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~PARKING_SPACE_QTY+SALES_AREA_SIZE_NUM+SALES_AREA_SIZE_NUM*PARKING_SPACE_QTY, data= store_data[-c(58,27),c(7,8,9)])
summary(Store_P_A1)
summary(Store_P_A1)
summary(Store_P_A1)
plotForecastErrors(TS_data_train_ARIMA_forecast$residuals)
dev.off()
plotForecastErrors(TS_data_train_ARIMA_forecast$residuals)
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)])
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~PARKING_SPACE_QTY+SALES_AREA_SIZE_NUM+SALES_AREA_SIZE_NUM*PARKING_SPACE_QTY, data= store_data[-c(58,27),c(7,8,9)])
summary(Store_P_A1)
car::durbinWatsonTest(Store_P_A1)
Box.test(Store_P_A1$residuals, lag = 1, type = 'Ljung-Box')
par(mfrow=c(2,2))
plot(Store_P_A1)
dev.off()
acf(residuals,lag.max = 20)
pacf(residuals,lag.max = 20)
?acf
Store_P_A1_GLS = gls(AVG_WEEKLY_BASKETS~., data= store_data[-c(58,27),c(7,8,9)], correlation=corARMA(p=1,q=1))
summary(Store_P_A1_GLS)
?gls
TS_data_train_AutoCorr = acf(TS_data_train,lag.max = 50)
TS_data_train_PartialAutoCorr = pacf(TS_data_train, lag.max = 10)
Store_P_A1 = lm(AVG_WEEKLY_BASKETS~PARKING_SPACE_QTY+SALES_AREA_SIZE_NUM+SALES_AREA_SIZE_NUM*PARKING_SPACE_QTY, data= store_data[-c(58,27),c(7,8,9)])
summary(Store_P_A1)
summary(Store_P_A1)
par(mfrow=c(2,2))
plot(Store_P_A1)
dev.off()
Store_P_A = lm(AVG_WEEKLY_BASKETS~PARKING_SPACE_QTY+SALES_AREA_SIZE_NUM+SALES_AREA_SIZE_NUM*PARKING_SPACE_QTY, data= store_data[,c(7,8,9)])
summary(Store_P_A)
summary(Store_P_A)
par(mfrow=c(2,2))
plot(Store_P_A)
?tbats
acf(TS_data_train, lag.max = 20)
dev.off()
acf(TS_data_train, lag.max = 20)
pacf(TS_data_train, lag.max = 20)
?tbats
